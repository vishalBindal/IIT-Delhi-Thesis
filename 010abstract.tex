Given a natural language instruction and an input scene, our goal is to train a model %which when presented with a natural language instruction, and an input scene, 
to output a \emph{manipulation program} that can be executed by the robot.
%%, composed of learned actions and reasoning on the input scene, 
%describing how the input scene can be manipulated by the robot resulting in the intended scene. 
%
%The inferred program also provides sequential sub-goals as end-effector positions for the robotâ€™s low-level motion planner to execute. 
%
Prior approaches for this task possess one of the following limitations: (i) rely on hand-coded symbols for concepts limiting generalization beyond those seen during training (ii) infer action sequences from instructions but require dense sub-goal supervision or (iii) 
lack semantics required for deeper object-centric reasoning inherent in interpreting complex instructions.
%
%From a modeling perspective, we first parse the natural language command in the form of a symbolic program, which is then executed over the input scene, using the neural representation of various symbolic concepts.
%
%The model learns grounded and executable neural representations for actions (as a transformation to objects in a latent space). 
%
% In contrast, our approach is neuro-symbolic and can handle linguistic as well as perceptual variations, is end-to-end differentiable requiring no intermediate supervision, and makes use of symbolic reasoning constructs which operate on a latent neural object-centric representation, allowing for deeper reasoning over the input scene.
In contrast, our approach can handle linguistic as well as perceptual variations, end-to-end trainable and requires no intermediate supervision. The proposed model uses symbolic reasoning constructs that operate on a latent neural object-centric representation, allowing for deeper reasoning over the input scene.
%, resulting in an expressive space of grounded manipulation programs.
%that are generalizable to unseen settings. 
%that are highly 
%interpretable and generalizable to unseen settings. 
%
Central to our approach is a modular structure  consisting of a {\em hierarchical instruction parser} and an {\em  action simulator} to learn  disentangled action representations.
%learned via RL and the use of \emph{gumble-softmax} layer to work as a selector facilitating \emph{disentanglement} in the action space. 
%
%Overall model is differentiable end-to-end with no intermediate supervision. 
%
%
Our experiments on a simulated environment with a 7-DOF manipulator, consisting of instructions with varying number of steps and scenes with different number of objects,  demonstrate that our model is robust to such variations and significantly outperforms baselines, particularly in the generalization settings.


Next, we observe that the ability to automatically detect and recover from failures is an important but challenging problem for an autonomous robot due to the presence of perceptual noise and uncertainty in discovering the goal state.
%
While existing approaches deal with this problem by training over data specifically annotated for failures, our goal in this work is to achieve this task, in a self-supervised manner, without requiring any such explicit annotation. 
%
Central to our approach is a neuro-symbolic object-centric representation of the state in the form of a dense scene graph. The core learning component of our approach consists of (a) neural module representing the transition function trained using data generated from an existing neuro-symbolic planner and (b) neural discriminators, trained in self-supervised manner, which not only detect failures, but also provide useful information about which objects in the scene need to be acted upon, localising the error and aiding an efficient discrepancy-aware synthesis of a recovery plan.  
%
Our technique is simple: (a) At any given execution step, we use a combination of our learned transition function and discriminator to estimate if a failure has occurred and also determine objects that need to be moved to recover from the failure (b) a recovery mechanism searches for an efficient plan to one of the states lying on the original plan %the state where failure had occurred 
using a forward search guided with learned heuristics.  
%
% When the last correct state may not be the closest to the failure state, we devise an anytime version of our algorithm to search for the optimal point on the original plan to the goal.
%
Experiments on a simulated data set with a variety of failures during plan execution shows the effectiveness of our approach compared to existing baselines.
