\section{Evaluation and Results}\label{sec:evaluation}
%
~\textcolor{red}{q. this para can be removed for shortening perhaps}In our experiments we study (i) whether our method can infer 
programs to translate instructions to desired goal states, 
(ii) the extent to which the method generalizes to novel 
instructions and world states and (iii) whether the model 
can generalize to multiple step plans having been trained on 
simpler plans and (iv) whether model predictions can be used by a simulated 
robot to execute a task as well as hypothesize future world states. 

% Main accuracy results
\begin{table*}[ht]
    \centering
    \caption{Accuracy Comparison for the Proposed Model and the Baseline.  (BB: Bounding Boxes)}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
         Model  & \multicolumn{3}{|c|}{Overall} &  \multicolumn{2}{|c|}{Single-step} & \multicolumn{2}{|c|}{Double step} & \multicolumn{2}{|c|}{Simple} & \multicolumn{2}{|c|}{Complex} \\ 
         \hline
         \hline
          & IOU & IOU-M & Program & IOU & IOU-M  & IOU & IOU-M  & IOU & IOU-M & IOU & IOU-M  \\
           &  &  & (Action/Subj/Pred) & &   &  & &  &  &  &   \\
          \hline
         Baseline (Gold BB) & 0.77 & 0.55 & --/0.81/0.76 & 0.80 & 0.56 &    0.71 & 0.52 &0.90 & 0.71 & 0.64 & 0.31\\ 
         \hline 
         Ours (Gold BB)  & 0.87 & 0.72& 0.99/0.99/0.94 & 0.91 & 0.64 &   0.87 & 0.62 & 0.92 & 0.73 & 0.87 & 0.64 \\
        \hline 
         Baseline (extracted BB) & 0.69 & 0.32& --/0.81/0.55 & 0.78 & 0.49 &  0.58 & 0.22 & 0.79 & 0.55 & 0.60 & 0.20\\ 
         \hline 
         Ours (extracted BB) & 0.76  & 0.41  & 0.99/0.74/0.76 &0.80 & 0.43  & 0.69 & 0.54  & 0.83 & 0.62 & 0.69 & 0.24\\
         \hline 
    \end{tabular}
    % \vspace{1em}
    \label{tab:accuracy}
\end{table*} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% EXPERIMENTAL SETUP %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{table*}[ht]
%     \centering
%     \caption{Accuracy Comparison for the Proposed Model and the Baseline}
%     \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
%     \hline
%          Model  & \multicolumn{3}{|c|}{Overall} &  \multicolumn{3}{|c|}{Single-step} & \multicolumn{3}{|c|}{Double step} & \multicolumn{3}{|c|}{Simple} & \multicolumn{3}{|c|}{Complex} \\ 
%          \hline
%          \hline
%           & IOU & IOU-M & & Program & IOU & IOU-M & Program &  IOU & IOU-M & Program & IOU & IOU-M & Program & IOU & IOU-M  & Program \\
%           \hline
%          Baseline & 0.77 & 0.55 & 0.80 & 0.56 & 0.71 & 0.52 & 0.90 & 0.71 & 0.64 & 0.31\\ 
%          \hline 
%          Ours  & 0.87 & 0.72 & 0.91 & 0.64 & 0.87 & 0.62 & 0.92 & 0.73 & 0.87 & 0.64 \\
%         \hline 
%          Baseline + Perception & 0.69 & 0.32 & 0.78 & 0.49 & 0.58 & 0.22 & 0.79 & 0.55 & 0.60 & 0.20\\ 
%          \hline 
%          Ours + Perception & 0.76  & 0.41  & 0.80 & 0.43 & 0.69 & 0.54 & 0.83 & 0.62 & 0.69 & 0.24\\
%          \hline 
%     \end{tabular}
%     % \vspace{1em}
%     \label{tab:accuracy}
% \end{table*} 



\begin{table}
    \centering
    \caption{Comparison of Proposed Model and CLIPort}
    \begin{tabular}{|c|c|c|}
         \hline 
         Model  & Identification Precision & Placement Precision \\ 
         \hline
          CLIPort &  &    \\
          \hline
         CLIPort(x5) &  & \\ 
         \hline 
         Ours    & 0.93 & 0.81 \\
        \hline 
    \end{tabular}
    % \vspace{1em}
    \label{tab:accuracy}
\end{table} 
\subsection{Experimental Setup}
%%% DATA %%%
\textbf{Data collection. } 
We perform evaluation in a PyBullet table top environment using a 
simulated Franka Emika Panda robot arm capable of picking and placing 
an object in its reachable workspace to a Cartesian coordinate. 
%
The workspace consists of a table top with 
blocks of different categories and colors that can be placed to the 
left, right or on top of another object.
%
Synthetic scenes are sampled with $3-5$ blocks (with variations in color and type attribute) and placed at orientations randomized in an angular range of $10$ degrees in the $(x,y)$-plane along the robot base. 
%
An evaluation data set consisting of a language instruction 
and a corresponding demonstration of the task is collected. 
%
The procedure above resulted in a data set of size $5000$ with each data point consisting of the initial scene, final scene and a language instruction without any sub-goal supervision. 
%
The instructions can range from those conveying a single physical interaction \emph{``put the red block on the green block"} or an extended interaction involving a composition of interactions such as \emph{``Put the red block which is behind the white dice to right of the blue cube, then put the lego block on top of red block"}. 
%
%The robot's motion skills are realized using a crane grasping for
%picking and re-positioning the object at the target pose.

%%% BASELINE %%%
\textbf{Baselines} We compare our model against the following two representative baselines. \\ 
%
\textcolor{blue}{
\textbf{CLIPort}~\cite{shridhar2022cliport} performs language-guided manipulation by leveraging the visual and linguistic semantics embedded in pre-trained CLIP~\cite{clip}. Unlike ours, the model lacks explicit, modular constructs to perform the reasoning inherent in object manipulation tasks.} ~\textcolor{red}{TODO: Add explanation for table3} \\ 
\textbf{Neural-only baseline} is adapted from \cite{andreas2016neural}. We provide an object-centric world representation to the model without assuming known concept grounding or sub-goal supervision for comparison with our setting. For a fair evaluation, our model and the baseline share the instruction encoder, the action simulator and the object feature extractor. The baseline includes an action decoder and four attention modules divided into two groups of two each. Each group takes in the encoded instruction and embedding for all objects present in scene and predicts two objects involved in manipulation. The location of the predicted objects and output of the action decoder is the input to the action decoder, that provides the predicted location as output. Here, \textit{location} includes both the bounding box and depth of the object. The entire architecture is neural and is trained end-to-end. %described in Section~\ref{subsec:loss}.
% \textbf{Neural-only baseline} is adapted from \cite{andreas2016neural}. We provide an object-centric world representation to the model without assuming known concept grounding or sub-goal supervision for comparison with our setting. For a fair evaluation, our model and the baseline share the instruction encoder, the action simulator and the object feature extractor. The baseline includes an action decoder and four attention modules. The action decoder acts on the encoded instruction and gives a dense representation for the action that needs to be executed. On the other hand, the attention modules are divided into two groups of two each. Each group takes in the encoded instruction and embedding for all objects present in scene to give a probability distribution over the objects. These distributions are then used to get the location of the two objects involved in manipulation as a weighted linear combination of the locations of the objects in the scene. We then pass the action representation as obtained from the decoder, and the two object locations to the action simulator to get the predicted location. Here, \textit{location} includes both the bounding box and depth of the object. The entire architecture is neural and is trained end-to-end via back propagation of the loss. %described in Section~\ref{subsec:loss}.
\begin{figure}
\begin{subfigure}{1.0\hsize}
     \centering    
     \includegraphics[scale=0.19]{figures/baseline.png}
    \label{fig:large_scenes}
\end{subfigure}%
\caption{Baseline Model}
\end{figure}

%%% METRICS %%%
\textbf{Metrics.}
%
The following metrics are used for evaluation: 
(i) \emph{Intersection over Union (IOU)}: is used as a metric for evaluating the locations of the predicted bounding boxes in the final scene in comparison with the ground truth bounding boxes extracted from the original demonstration. The IOU metric is calculated in 2D in the image space (assuming a static camera viewing the scene). Average IOU over all objects in the scene and mean IOU for objects moved during execution, termed IOU-M is reported.  
(ii) \emph{Program Accuracy}: The grounded program inferred for an (instruction, scene)-pair using the proposed model is compared with the ground truth program (manually annotated). We separately report the grounding accuracy for the subject and predicate of our action (assumed binary) and the accuracy of the predicted action inferred from the instruction. Since, there is no explicit notion of grounded actions in the baseline, we do not report this metric for the baseline. 

~\textcolor{red}{Add metric corresponding to CLIPort}
\emph{CLIPort} predicts pixel-level attention masks for pick and place locations. For a fair comparison, we use the following metrics 
(iii) \emph{Placement Precision:} For CLIPort placement precision refers to the fraction of test examples in which the predicted place location lies within the groundtruth bounding box of the moved object in the final scene. For our model, it refers to the fraction of test examples in which the center of the predicted bounding box lies within the groundtruth bounding box of the moved object
(iv) \emph{Identification Precision:} For CLIPort identification precision refers to the fraction of test examples in which the predicted pick location lies within the groundtruth bounding box of the moved object in the initial scene. For our model, it refers to the fraction of test examples in which the correct object to be moved is identified by the \emph{Visual Reasoner}

%%% SETTING %%%
% \textbf{Quantitative Evaluation}: This is carried out in two settings. First, we use a $80:20$ train:test split of the entire corpus for accuracy comparison. Second, for evaluating the approaches in a generalization setup, a different split is used where the model is evaluated on a generalization set consisting of novel instructions and scenes not seen in training. Finally, generalization is also carried out in the space of plans where the model is trained on instructions conveying one or two actions and evaluated on instructions that involves larger number of steps for carrying out the task. 


%
%% Generalization on Larger Scenes
\begin{figure}
\begin{subfigure}{0.5\hsize}
     \centering    
     \includegraphics[scale=0.19]{figures/multi-object.png}
    \caption{\footnotesize{IoU vs \# of objects}}
    % IoU of the predicted position of the object moved during execution with its ground truth position v/s number of objects in the scene.
    \label{fig:large_scenes}
\end{subfigure}%
% Generalization on larger steps
% \begin{figure}[!h]
%     \centering
%     \subfigure{\includegraphics[width=0.2\textwidth]{figures/large_steps.png}} 
%     \subfigure{\includegraphics[width=0.2\textwidth]{figures/generalization_large_scenes.png}} 
%     \caption{Caption}
%     % \label{fig:foobar}
% \end{figure}
\begin{subfigure}{0.5\hsize}
     \centering    
     %\includegraphics[height=4.375cm,width=3cm]{figures/multi-step.png}
    \includegraphics[scale=0.19]{figures/multi-step.png}
    \caption{\footnotesize{IoU vs varying \# of steps}}
     %IoU of the predicted position of the object moved during execution with its ground truth position v/s number of atomic actions implied in the instruction.
    \label{fig:large_steps}
\end{subfigure}
\caption{Performance in Generalization Settings}
\end{figure}

\begin{figure*}[hbt!]
    \centering    
    \includegraphics[width=17cm]{figures/row2a.png}
    % \caption{
    % \footnotesize{
    % }}
    \label{fig:qual-1}
\end{figure*}

\begin{figure*}[hbt!]
    \centering    
    \includegraphics[width=17cm]{figures/row2b.png}
    \caption{
    \footnotesize{
    Execution of robot manipulator on (a) compound instructions, (b) scene with 15 objects, (c) double step instruction with relational attributes, (d) 5-step instruction. (d) also shows reconstruction of the predicted scene before each step of the simulation
    }}
    \label{fig:qual-1}
\end{figure*}


%% Main Results
%%%%%%%%%%%%%%%
\subsection{Model Accuracy}
% Quantitative evaluation
%\textbf{Quantitative Evaluation.} 
A quantitative evaluation of model accuracy is is carried out in two settings. First, we use a $80:20$ train:test split of the entire corpus for accuracy comparison. The corpus consists of both single step and double step commands, along with sentences of different complexities:- \textit{simple} and \textit{complex}. Complex sentences involve reasoning on inter-object relationships, while simple sentences reason over individual object features only. Second, to evaluate the approaches in a generalization setup, a different split is used where the model is evaluated on a generalization set consisting of unseen instructions and scenes not seen in training. Finally, generalization is also carried out in the space of plans where the model is trained on instructions conveying one or two actions and evaluated on instructions that involves larger number of steps for carrying out the task. 

% %%% ACCURACY %%%
% \subsection{Accuracy Results}
% \begin{table}
%     \centering
%     \caption{Effect of using Extracted boxes}
%     \begin{tabular}{|c|c|c|c|}
%     \hline
%          Model & Action & Subject & Predicate  \\ 
%          \hline
%         Ours & 1.0 & 0.97  &  0.95  \\ 
%         \hline
%        Ours + Perception & & \textbf{0.66} & \textbf{0.88}\\ 
%        \hline
%     \end{tabular}
%     % \vspace{1em}
%     \label{tab:prog_accuracy}
% \end{table} 
% %
We demonstrate our model's ability to infer a sequence of grounded sub-goals, given an initial scene and instruction, that involves manipulating the scene over multiple time step and complex multi-hop reasoning.
%
Table \ref{tab:accuracy} reports the performance of our model and the baseline on the test set. We report both numbers: using gold set of bounding boxes and using bounding boxes extracted~\footnote{Excluded about 10\% no detection cases for both models.} using the approach described in Section~\ref{subsec:visual-reason}. Our model outperforms the baseline overall. For instructions with complex reasoning (resolution of binary spatial relations) involved, the proposed model outperforms the baseline by $33$ points in the IOU-M metric.
%
Disentangled representations of relations and concepts in \textit{Visual Reasoning} module allow the proposed model to reason over complex instructions. For the remaining experiments, the numbers are reported using gold BBs for both the models.
% \texttt{Single-step} and \texttt{Double Step} refer to instructions with one and two actions involved resprectively and \texttt{Simple reasoning} refers to simple reasoning bases upon 
% reports the accuracy results. Both test (around 1700 samples) and train set (around 1700 samples) contain 1 - 2 step commands. Further, test and train do not have any 1-step command in common. Both models achieve a high accuracy during training but the proposed model shows significant improvement over the baseline for the test set.  
%
% Overall Accuracy on Test and Train Sets
%\subsection{Generalization With Increasing Scene Objects}
% The proposed model is able to interpret the correct object and move it to the correct position with marginal decline in performance to scenes with up to 10 objects after being trained on scenes with up to 5 objects only, without any obtrusive decrease in accuracy. Disentangled representations of object features and spatial features in the scene graph extracted by the \textit{Visual Reasoner} from the scene along with an object-centric view of the environment help the model to generalise strongly to larger number of objects. 


%%%%% Generalization
\subsection{Model Generalization} 
Figure~\ref{fig:large_scenes} illustrates the performance of the model on scenes with larger number of objects than seen in training. 
%
The proposed model was first trained on scenes having up to $5$ objects only, and then tested on scenes having up to $10$ objects. 
%
Even on larger scenes, the model is able to interpret the correct object and move it to the correct position with marginal decrease in accuracy. 
%
The improved generalization demonstrated by the model can be attributed to reliance on an object-centric world model and the ability to learn dense disentangled representations for spatial and action concepts facilitating modular and structured reasoning that scales gracefully with increasing scene complexity.  
Next, we evaluate model generalization to inferring plans extending to time horizons beyond those observed during training. For this experiment, the model is trained on instructions conveying plans with up to few ($1$-$2$ step) manipulation actions and evaluated on instructions with 
longer action sequences. 

Figure ~\ref{fig:large_steps} compares generalization performance for the proposed and the baseline models. We observe that the proposed model is able to perform multiple scene manipulation and reasoning steps with considerable accuracy up to $7$ steps after being trained with instructions translating to plans with up to $2$ steps. 
%
The performance of the neural-only baseline is worse and the model struggles to generalize to plans extending to longer horizons. We attribute this to the modular structure of our approach compared to the baseline.
%
% The improved generalization performance of the proposed model can be attributed to the ability to reason in a rich space of latent programs. 
%
% The ability to learn a modular and disentangled space of actions facilitates allows complex programmatic reasoning over an extended period of time. 
%
%In contrast, the baseline does not have access to the latent space of programs, burdens the learner to learn a full transition model which does not scale well over time. 

% Qualitative Results














\iffalse
\begin{figure}[h]
    \centering    
    \includegraphics[width=7.5cm]{figures/rel.png}
    \caption{
    \footnotesize{Execution of robot manipulator on a double step instruction containing relational attributes: \emph{Place the thing at the right of the red lego object to the right of the green dice and put the cyan lego block on top of the cyan dice}
    }}
    \label{fig:relative}
\end{figure}
\fi
%
\subsection{Demonstration on a Simulated Robot}
%
We demonstrate the learned model for interpreting instructions 
provided to a simulated 7-DOF Franka Emika manipulator in a table top setting. 
%
The robot is provided language instructions and uses the model to predict a program that once executed transitions the world state to the intended one. The 2-D bounding boxes predicted by the action simulator are translated to 3-D coordinates in the world space via a learnt MLP using simulated data.
%
The predicted positions are provided to a low-level motion planner for trajectory generation with crane grasping for picking/placing. 
%Each step of the robot simulation is then performed by grasping the object at the initial location, moving the gripper to the final predicted location, and releasing the gripped object. 
% 
Figure \ref{fig:qual-1} shows execution by the robot manipulator on complex instructions, scenes having multiple objects, double step relational instructions, and multi-step instructions. We also visualise reconstruction of the moved objects before each step of the actual execution. The structural similarity index (SSIM) for the reconstruction model is 0.935.
% omitted since we do not have space
%Figure \ref{fig:relative} demonstrates model performance on an instruction containing relational attributes.
%



\iffalse
\subsection{Failure cases}
In roughly 10\% of examples in the dataset, the object detector returns a false object proposal, or misses an object, in either of initial or final scenes. Since we are unable to calculate loss in such cases, these examples are not considered for experiments with perception.
\fi

%%%% ATTIC %%%%
\iffalse

% Table~\ref{tab:accuracy} reports the performance of our model and the baseline on the test set. 
% reports the accuracies for the proposed and the 
% baseline models. The results indicate a stronger generalization for the proposed approach compared to the baseline model.
%
% The result illustrates the modelâ€™s ability to reason about language instructions with novel object attribute references in the context of the scene. Such visual-linguistic reasoning operates on a single scene. Next, we assess generalization over multiple time steps. We explicitly evaluate the model on a test cases where an instruction involves multi-step actions that exceed those seen in training.

% %  \vspace{-2em}
% \begin{table}
%     \centering
%     \caption{Generalization On Instructions with Novel Object Attributes}
%     \begin{tabular}{|c|c|c|c|c|c|c|c|}
%     \hline
%     Model & Prog ($P$) & Arg ($O_1$) & Arg ($O_2$) & IOU-2D & IOU-3D \\ \hline \hline
%     Baseline & - & 87.5 & 89.1 & 29.0 & 16.6 \\ 
%     Ours &  98.5 & 96.3 & 96.3 & 51.6  & 25.2  \\
%    \hline
%     \end{tabular}
%     \label{tab:generalization-attributes}
% \end{table}
% %
%  \vspace{-0.5em}
% \begin{table}
%     \centering
%     \caption{Generalization over Three-step Instructions with One/Two-step Instructions in Training}
%     \begin{tabular}{|c|c|c|c|c|c|}
%     \hline
%     %Model &$P$ & ($P_g$) & ($o_1$) & ($o_2$) & IOU-2D & IOU-3D \\ 
%     Model & Prog ($P$) & Arg ($O_1$) & Arg ($O_2$) & IOU-2D & IOU-3D \\
%     \hline \hline
%     Baseline & - & 76.0  & 80.9 & 13.3 &  6.6  \\
%     Our & 56.8  & 54.7 & 53.6 & 27.2 & 12.9 \\
%     % Random & 12.5 & 0.09 & 3.4 & 3.4 & - & -\\
%     \hline
%     \end{tabular}
%     \label{tab:generalization-multi-step}
% \end{table}

% Table~\ref{tab:generalization-multi-step} presents the results. The models were only trained on 1-2 step commands, and are tested on 3 step commands. The proposed model shows improved generalization compared to the baseline in the IOU metrics. 

% %  \vspace{-2em}
% \begin{table}
%     \centering
%     \caption{Generalization to Novel Multi-Object Attribute Instructions}
%     \begin{tabular}{|c|c|c|c|c|c|}
%     \hline
%         Model & Prog ($P$) & Arg ($O_1$) & Arg ($O_2$) & IOU-2D & IOU-3D \\\hline \hline
%         %Model & ($P$) & ($o_1$) & ($o_2$) & IOU-2D & IOU-3D \\  \hline \hline
%         Baseline & - &55.0 &32.4 & 3.2& 1.5\\
%       Ours & 98.0 & 75.7 & 89.5 & 27.3 & 9.3 \\ \hline 
%     %   Baseline & 84.3 & 90.2 & 93.0 & 49.0 & 30.3     \\\hline
%     \end{tabular}
%     % \vspace{1em}
    \label{tab:generalization-multiple-object-attributes}
% \end{table}

% Table~\ref{tab:generalization-multiple-object-attributes} presents evaluation on task instructions that involve multi-step reasoning over attributes in the setting where the model is trained 
% only on a single attribute. For example, the training set instructions only involve singe 
% attributes such as 'red' and 'lego' with two different objects but during testing the model must compositional reason over both attributes to understand and follow the instruction. As before, 
% the ability to combine grounded concepts with reasoning allows the proposed model to generalize 
% better in this setting achieving significantly higher generalization accuracy compared to the baseline. 

% The above results demonstrate stronger generalization to novel scenes with unseen object attribute pairs as well as action sequences that extend beyond those encountered during training. 
% %
% The neuro-symbolic approach involves learning of data-driven learning of concept representations that are amenable to rich compositional reasoning. The neuro-symbolic approach transfers better to novel settings compared to the direct neural-only approach that does not make use of the modular structure during training.

\begin{figure}[ht]
    \centering
    \setlength\tabcolsep{1.5pt}
    \begin{tabular}{ccc}
      \includegraphics[width=28mm]{figures/1step/rgba0002.png} &   \includegraphics[width=28mm]{figures/1step/rgba0009.png} &
      \includegraphics[width=28mm]{figures/1step/rgba0017.png}\\
    % \multicolumn{2}{c}{\includegraphics[width=55mm]{it} }\\
    % \multicolumn{2}{c}{(e) fifth}
    \end{tabular}
    \caption{A robot manipulator correctly performing a single step instruction \emph{``put cyan block on blue block"} in a simulated table top environment. }
    \label{fig:demo-1}
\end{figure}

%
Figure~\ref{fig:demo-1} illustrates the robot successfully performing a single step instruction, \emph{``put cyan block on the blue block"} using the sub-goal predicted by the inferred program. 

%
Figure~\ref{fig:demo-2} illustrates the robot manipulator performing the instruction \emph{``put yellow block to the left of white block and put magenta small block on yellow block"}. the model reasons about the input instruction in the context of the environment correctly positions the yellow block on the left side of the intended white block. Subsequently, the robot grasps the magenta block and correctly positions the block on the yellow block that was previously manipulated. 

\begin{figure}
    \centering
    \setlength\tabcolsep{1.5pt}
    \begin{tabular}{ccc}
       \includegraphics[trim={5cm 5cm 5cm 0},clip,width=28mm]{figures/2step_s/rgba0002.png}  &
       \includegraphics[trim={5cm 5cm 5cm 0},clip,width=28mm]{figures/2step_s/rgba0006.png}  &
       \includegraphics[trim={5cm 5cm 5cm 0},clip,width=28mm]{figures/2step_s/rgba0011.png} \\
       \includegraphics[trim={5cm 5cm 5cm 0},clip,width=28mm]{figures/2step_s/rgba0015.png} &
       \includegraphics[trim={5cm 5cm 5cm 0},clip,width=28mm]{figures/2step_s/rgba0020.png} &
       \includegraphics[trim={5cm 5cm 5cm 0},clip,width=28mm]{figures/2step_s/rgba0021.png} 
    \end{tabular}
    \caption{Robot manipulator performing a two-step instruction \emph{``put yellow block to the left of white block and put magenta small block on yellow block"}}
    \label{fig:demo-2}
\end{figure}
%Further, note that the model used in the demonstrates shown in 
%Figures~\ref{fig:demo-2} and~\ref{fig:demo-2} 

% Incorrect: multi step instruction. 
\begin{figure}
    \centering
    \setlength\tabcolsep{1.5pt}
    \begin{tabular}{ccc}
       \includegraphics[trim={5cm 5cm 5cm 0},clip,width=28mm]{figures/2step_f/rgba0002.png}  &  
       \includegraphics[trim={5cm 5cm 5cm 0},clip,width=28mm]{figures/2step_f/rgba0008.png}  &
       \includegraphics[trim={5cm 5cm 5cm 0},clip,width=28mm]{figures/2step_f/rgba0016.png} \\
       \includegraphics[trim={5cm 5cm 5cm 0},clip,width=28mm]{figures/2step_f/rgba0026.png} &
       \includegraphics[trim={5cm 5cm 5cm 0},clip,width=28mm]{figures/2step_f/rgba0031.png} &
       \includegraphics[trim={5cm 5cm 5cm 0},clip,width=28mm]{figures/2step_f/rgba0034.png} \\
    %   \multicolumn{3}{c}{Robot manipulator performing a double step instruction \emph{"put red block on blue block and put magenta block on red block"}. The robot correctly inferred the sequence of symbolic actions but failed during execution error due to the unstable placement of the second block. }
    \end{tabular}
    \caption{Robot manipulator performing a two-step instruction \emph{"put red block on blue block and put magenta block on red block"}. The robot correctly inferred the sequence of symbolic actions but failed during execution error due to the unstable placement of the second block. }
    \label{fig:demo-3}
\end{figure}

%
Figure~\ref{fig:demo-3} shows the execution for the instruction \emph{``put red block on blue block and put magenta block on red block"}. The model correctly predicts a program consisting of a composition of two sequential manipulation actions. The robot correctly identifies the intended objects for manipulation, then grasps and re-positions them to form the intended assembly. In the final step, the robot correctly placed the magenta-colored block on top of the red-colored block, however, the inferred placement was unstable and the block fell from the assembly. Note that the current model infers the task plan which is then handed over to the motion planner. Such a staged approach possesses the inherent limitation of not being able to recover from execution failures. The possibilities of a reactive approach by interleaving planning and execution are to be explored in future work and is likely to benefit error-recovery.

%
In Figure~\ref{fig:demo-4}, the robot is provided with the instruction, \emph{``put red block to the left of yellow block and put cyan block to the left of red block and then put red block on cyan block"} which requires three successive manipulations. The model used in this experiment is trained only on two step instructions. However, at inference time, the model can generalize to a longer instruction by leveraging the compositionality inherent in our program representation. 


\begin{figure}
    \centering
    \setlength\tabcolsep{1.5pt}
    \begin{tabular}{ccc}
       \includegraphics[trim={5cm 5cm 5cm 0},clip,width=28mm]{figures/3step_s/rgba0002.png} &  
       \includegraphics[trim={5cm 5cm 5cm 0},clip,width=28mm]{figures/3step_s/rgba0011.png}  &
       \includegraphics[trim={5cm 5cm 5cm 0},clip,width=28mm]{figures/3step_s/rgba0015.png} \\
       \includegraphics[trim={5cm 5cm 5cm 0},clip,width=28mm]{figures/3step_s/rgba0020.png} &
       \includegraphics[trim={5cm 5cm 5cm 0},clip,width=28mm]{figures/3step_s/rgba0025.png} &
       \includegraphics[trim={5cm 5cm 5cm 0},clip,width=28mm]{figures/3step_s/rgba0030.png} \\
    \end{tabular}
    \caption{Robot manipulator performing a three-step instruction \emph{``put red block to the left of yellow block and put cyan block to the left of red block and then put red block on cyan block"}. The robot correctly inferred the sequence of actions. }
    \label{fig:demo-4}
\end{figure}
\fi


% \begin{figure*}[hbt!]
%     \centering    
%     \includegraphics[width=15cm]{figures/qualitative.png}
%     \caption{
%     \footnotesize{Execution of robot manipulator on a 5-step instruction: \emph{Put the blue lego thing on the left side of the red lego thing and place the red cube on the left side of the white lego object and move the magenta dice to the right of the green box and move the red box on the left side of the blue lego thing and put the blue lego object to the left of the white lego thing}. The first row shows the scene after each action in the robot simulation. The second row shows reconstruction of the predicted scene before each step of the simulation
%     }}
%     \label{fig:qualitative}
% \end{figure*}
