\section{Introduction}\label{sec:intro}
An important aspect of automated planning and execution is recovery from errors. While plans may be perfect in simulation, there are often marred by imperfections in the real world caused by noise in the sensory inputs, motor failures, or unexpected collisions. This results in deviations from the original path of planned execution, which the agent needs to recover from. Often, the considerations of efficiency both in terms of time it takes to recover from the error, in terms of re-planning, as well as the time it takes for the new plan to reach the goal state, both become important. Further, even before recovery can happen, it is imperative that failures be correctly detected and flagged. Classical literature has looked at this problem in a purely symbolic set-up, where the discovery simply corresponds to a goal check function, and recovery assumes the form of re-planning to the desired sub-goal -- either directly to the original goal, or to the point of last deviation from the goal. 

More recently, literature has looked at models which deal with perceptual data, containing various kind of noise, resulting in corresponding models for error recovery which account for variations in the input. Several of these approaches require hand annotated datasets, to learn a supervised function for discriminating two states from each other, both for error recovery, as well as re-planning. Creating such annotations, especially for failures, may be both time-consuming and expensive, in terms of the human effort involved. In this work, we take a different approach to this problem, and ask the following question "Is there a way to learn a state discriminator function in a self-supervised manner which does not required hand annotated data for failed states?". An affirmative answer to this question would presumably help in building a cost-effective and scalable error recovery mechanism. Further, even after the failure has been detected, we would like to be efficiently able to reach the original goal while minimizing the time taken to re-plan, which can be prohibitive~\cite{fox2006plan}. This requires doing an efficient plan search to the desired sub-goal, which can exploit the localised information about where the failure might have occurred. 

Motivated by these questions, we propose an efficient approach for discrepancy aware failure recovery mechanism based on an object-centric scene graph representation of each state. Our state representation is decomposed in terms of individual object representations, which enables the agent to quickly discover what part of the state need to be changed to achieve the desired sub-goal, which is often the last state with correct execution of the original plan. Central to our approach is a set of neural discriminators to separate out two different state as well as object representations trained in a self-supervised manner. Our discriminator not only detects failure, but also provides localised information about which objects have caused the failure by discriminating object representations. %This helps in designing an efficient search mechanism as seen below.
%We use the scene graph and object representations seen earlier during the execution as negatives while training our discriminators, obviating the need for annotated data for failure discovery, which is an important characteristic of our approach. Since our discriminators are object-centric, they additionally provide as localized information about which objects are not in the right position in the scene, by comparison with the desired goal. This helps us design efficient heuristics while searching for the sub-goal during the error recovery phase. Finally, often when dealing with significant errors, the failed state may be far-off from the last correct state, and it may be more optimal to reach for another closer state on the original path to the goal, rather than simply reverting back to the previous correct state. Following this intuition, we design an anytime version of our algorithm, which given a budget, discovers the closest sub-goal which is on the path of the original plan to the goal. 

Our algorithm proceeds in the following steps. (a) We first train our neural discriminators using positive examples from the simulation, and negative examples generated using states seen during execution other than the current state. (b) Once the discriminators have been trained, after every step of the execution, we examine whether the resulting state is as desired (based on simulated plan) or if a failure has occurred. (c) In the former case, the execution proceeds as planned, whereas in case of an error, the recovery mechanism kicks in. (d) The search for recovery plan to the sub-goal, which is the last correct state, focuses on those objects which are the cause of the error as per the information provided by discriminator and manipulate these objects ignoring others. This results in an extremely efficient version of the A* search algorithm. (e) Once the recovery plan is found, the agent executes it to get back to the original path of execution to the goal. (f) In a variation of our approach, we sub-goal on a union of states which are on the path to the goal, and are among the top k states, based on a heuristic distance from the current state, resulting in an anytime version of our algorithm.  This is important to deal with cases when the last correct state may be far-off from the state resulting from the failure. 
%This results in an anytime version of our algorithm.


%Our discriminator not only detects failure, but also provides localised information about which objects have caused the failure by discriminating object representations. This helps in designing an efficient search mechanism as seen below.
%


%Our algorithm proceeds in the following steps. (a) We first train our neural discriminators using positive examples from the simulation, and negative examples generated using states seen during execution other than the current state. (b) Once the discriminators have been trained, after every step of the execution, we examiner whether the resulting state is as desired (based on simulated plan) or if a failure has occurred. (c) In the former case, the execution proceeds as planned, whereas in case of an error, the recovery mechanism kicks in. (d) The search for recovery plan to the sub-goal, which is the last correct state, focuses on those objects which are the cause of the error as per the information provided by discriminator and manipulate these objects ignoring others. This results in an extremely efficient version of the A* search algorithm. (e) Once the recovery plan is found, the agent executes it to get back to the original path of execution to the goal. (f) In a variation of our approach, we sub-goal on a union of states which are on the path to the goal, and are among the top k states, based on a heuristic distance from the current state. This results in an anytime version of our algorithm.

In order to demonstrate the efficacy our proposed approach, we create a dataset of simulated failures, such as collisions, mechanical failures and random perturbations of objects. We experiment with both single as well as multiple failures in a given plan. We work with the recently proposed neuro-symbolic model~\cite{Kalithasan2022LearningNP} as our base planning model. We compare with two different baseline (1) an RL based approach which has been trained to reach the goal in a reactive manner (b) a na\"{i}ve implementation of A* search to the sub-goal which does not exploit object-centric information about localization of errors. In both the cases, our proposed method is able to do significantly better in terms of time taken to reach the sub-goal for recovery
%~\footnote{Our approach is not comparable with RL based approach on this metric since it does reactive planning.}, as well as the length of the plan generated to reach there.


%Last few years have seen significant advancement in the capabilities of robots to perform various tasks of interest, often guided by natural language instruction. A large number of these systems are tested in simulation, but execution in the real world presents altogether different kinds of challenges, such as failure of the robotic arm to affect the desired action, object collisions during movement causing disruptions, and unexpected movement because of exogenous sources etc. To the best of our knowledge, there is very limited work which explicitly deals with these challenges, in the context of recovery from execution failures of the plans constructed by modern day deep neural models. In this work, we take a neuro-symbolic object-centric scene graph based approach to this problem, and propose a mechanism for recovery using a dynamic "discover, re-plan and act" approach.  Our approach consists of the following 3-steps (a) The first step is {\em Failure identification}, which proceeds by using neural discriminators between the simulated scene graph and graph corresponding to the scene obtained during actual execution. Our discriminators are trained in a self-supervised manner by using data from other scene graphs, which do not correspond to the current execution step, as negatives. (b) Once the failure is flagged, the {\em Recovery Plan} discovery mechanism kicks in. The re-planning proceeds using a version of A* search, since the knowledge of the goal state is known (via plan simulation). We make use of effective pruning strategies based on our object-centric representation to curtail down the search significantly. (c) Finally, the re-discovered plan is executed in a step by step manner, until the goal is accomplished, or another failure is encountered triggering another instance of the recovery mechanism. The entire system is trained end-to-end, without any intermediate annotations for failure discovery, or re-planning, and including learning of the pre-conditions for each of the actions. Experiments on a novel dataset consisting of real-world-like (simulated) failures demonstrates that our approach is both significantly more accurate as well as more efficient in failure discovery, and achieving the goal after re-planning, compared to existing SOTA baselines. We attribute our gains to the object-centric nature of our approach.