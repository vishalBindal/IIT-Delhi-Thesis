\label{sec:related}
%
Strategies for recovering from planning errors have been explored in the past and vary in terms of the underlying planning model and the objective of planning. 
%Here, we review few closely related efforts. 
% Classical Planning
Classical planning approaches assume an abstract symbolic domain model and and search for a feasible/optimal plan; often exploiting heuristics during  search~\citep{belta2007symbolic}.     
%
Errors during plan execution are detected by comparing the actual and expected world states. Works such as ~\citep{bercher2014plan,saetti2022optimising,fox2006plan} recover a plan from the erroneous state, \emph{re-planning} a new plan to the goal or \emph{re-pairing}, i.e., finding a plan close to the original plan, and satisfy the sub-goals (or \emph{obligations}) already fulfilled before an error was encountered.  
%
Such strategies inherit the same brittleness as symbolic planning when deployed on real robots due to the core assumption of noise-free observations of the state; causing inaccurate error detection and goal-checking. 

% Neural Planning
As an alternative, RL-based reactive planners learn neural policies that prescribe an action for a given state that the robot may encounter.   
%
Notable successes include learning manipulation skills ~\citep{rana2023residual,kumar2023graph,ebert2018visual} and tool use ~\citep{li2020towards,wu2019imagine} tasks that involve short multi-step reasoning. 
%
In this paradigm, error recovery is implicit in the learned policy, if during learning, the agent has explored the state space \emph{well enough} so as to  generalize to any state that the robot may encounter online. 
%
Such generalization is difficult in complex manipulation domains and hence RL-based reactive planners are used for local error recovery or in domains with simpler state spaces. 
%
The work of~\citep{ryu2022confidence,vatsefficient,thananjeyan2021recovery}, uses RL for local (short horizon) repair of learned skill policies. The use of RL allows the discovery of recovery behaviours improving over hand-coded strategies. The work of ~\citep{bagaria2020option} invokes hierachical-RL with the objective of covering a larger state space during training for better generalization.   
%
Whereas such approaches focus on myopic recovery required when skills such as grasping, pushing etc. fail, our work addresses plan repair over a longer horizon inherent in complex manipulation task (e.g., recovery from a fallen stack of blocks while creating an assembly). 

% Neuro-symbolic Planning
Recently, planning models have emerged which fuse neural representations with symbolic reasoning for generalized long-horizon planning~\citep{mao2022pdsketch,Kalithasan2022LearningNP,zhu2021hierarchical,xu2019regression,shridhar2022cliport,Mao2019TheNC}. 
%
Such planners allow data-driven learning of both spatial and action representations that can be composed for goal-directed reasoning. 
%
The problem of plan recovery in such models has received relatively less attention. This paper builds on a representative neuro-symbolic model and formally addresses the problem of error discovery and plan repair. 
%
Our work is also closely related to~\citep{sung2023learning}, who explore plan recovery in the context of task and motion planning problems that additionally consider planning in metric space. Their approach predicts the cause of plan failure and performs back jumping to construct a recovery plan. However, this approach uses supervised learning to predict the cause for a plan failure using supervised learning with a data set of failed plans. In contrast, our work ameliorates the need for a corpus of failures and instead performs learning only using successful goal-reaching demonstrations. 

% Removing for saving space.
%Other efforts have explored error recovery for specific robot planning problems. The work of ~\citep{molnar2023using} recovers from states where no goal-reaching motion plans are available, using meta-reasoning~\citep{griffiths2019doing} where alternative motion planners are invoked till a feasible plan is obtained. Our work builds on neuro-symbolic planners. The presence of learned spatial and action concepts grounded in metric space are used for fast sampling of feasible metric space instead of explicit motion planning while reasoning symbolically about the task. 
%
%Works such as \citep{sundaresan2021untangling} recover from hand-coded strategies for automated untangling of knots by proposing re-pose/re-centering moves when task progress is stalled. Works such as ~\citep{sharma2022correcting} and~\citep{knepper2015recovering,li2021reactive} invoke help from human operator for language guidance on how to perform task, to request for objects beyond the manipulation reach/capability of the robot or directly hand over control to complete a highly-dexterous task~\citep{galbally2022elly} beyond the robot's capability. This paper takes a first step in the direction of plan recovery for manipulation tasks in domains where rich inter-object interaction can occur. Extensions such as explaining plan failures~\citep{raman2013towards,raman2014unsynthesizable} or dialogue with a human operator for determining a recovery strategies \cite{banerjee2021robotslang,kim2018learning} remain part of future work.     
