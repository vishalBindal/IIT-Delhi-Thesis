\section{Introduction}\label{sec:intro}
An important aspect of automated planning and execution is recovery from errors. While plans may be perfect in simulation, there are often marred by imperfections in the real world caused by noise in the sensory inputs, motor failures, or unexpected collisions. This results in deviations from the original path of planned execution, which the agent needs to recover from. Often, the considerations of efficiency both in terms of time it takes to recover from the error, in terms of re-planning, as well as the time it takes for the new plan to reach the goal state, both become important. Further, even before recovery can happen, it is imperative that failures be correctly detected. Classical works have examined this problem in a purely symbolic setting, where the discovery simply corresponds to a goal check function, and recovery assumes the form of re-planning to the original goal, or repair to the sub-goal of last deviation from the plan.  

Alternate RL-based approaches learn a reactive policy that yield a high likelihood of reaching the goal from any state. However, they rely on well-enough offline exploration of the state space which is often difficult in complex manipulation domains. More recently, literature has examined models which deal with perceptual data, containing various kind of noise, resulting in corresponding models for error recovery which account for variations in the input. Most approaches require hand annotated data sets, to learn a supervised function for discriminating two states from each other, both for error recovery, as well as re-planning.
%
Creating such annotations, especially for failures, may be both time-consuming and expensive, in terms of the human effort involved. 
%
In this work, we take a different approach to this problem, and ask the following question "Is there a way to learn a state discriminator function in a self-supervised manner which does not required hand annotated data for failed states?". An affirmative answer to this question would presumably help in building a scalable error recovery mechanism. Further, even after the failure has been detected, we would like to be efficiently able to reach the original goal while minimizing the time taken to re-plan, which can be prohibitive~\cite{fox2006plan}. This requires doing an efficient plan search to the desired sub-goal, which can exploit the localised information about where the failure might have occurred. 

Motivated by these questions, we propose an efficient approach for discrepancy aware failure recovery mechanism based on an object-centric scene-graph based state representation. Our state representation is decomposed in terms of individual object representations, which enables the agent to quickly discover what part of the state need to be changed to achieve the desired sub-goal, which is often the last state with correct execution of the original plan. Central to our approach is a set of neural discriminators to separate out two different state as well as object representations trained in a self-supervised manner. Our discriminator not only detects failure, but also provides localised information about which objects have caused the failure by discriminating object representations. The search for recovery plan to the sub-goal, which is the last correct state, focuses on those objects which are the cause of the error as per the information provided by discriminator and manipulate these objects ignoring others. This results in a forward search accelerated with learned heuristics operating on a neuro-symbolic domain representation. 
 
To demonstrate the efficacy our proposed approach, we create a data set of simulated failures, such as collisions, mechanical failures and random perturbations of objects. We experiment with both single as well as multiple failures in a given plan. We work with the recently proposed neuro-symbolic model~\cite{Kalithasan2022LearningNP} as our base planning model. We compare with two different baseline (1) an RL-based approach which has been trained to reach the goal in a reactive manner (b) a na\"{i}ve implementation of A* search to the sub-goal which does not exploit object-centric information about localization of errors. In both the cases, our proposed method is able to do significantly better in terms of time taken to reach the sub-goal for recovery.
